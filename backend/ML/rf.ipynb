{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "random forest regressor: multiple inputs - multiple outputs \n",
    "- input\n",
    "  - user: Orig, Dest, 'depDay', 'arrDay'\n",
    "  - other attributes: distSeg1, distSeg2, distSeg3, segn, elaptime, detour, market_share, con1, con2, depDay2, depDay3, real_dist\n",
    "- output: pax, TOT_pax, cluster, dep_hour, dep_min, arr_hour, arr_min, stops, con_time\n",
    "\n",
    "Don't think about whether it's direct or not at the moment! Go simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orig</th>\n",
       "      <th>con1</th>\n",
       "      <th>con2</th>\n",
       "      <th>Dest</th>\n",
       "      <th>depDay2</th>\n",
       "      <th>depDay3</th>\n",
       "      <th>depDay</th>\n",
       "      <th>elaptime</th>\n",
       "      <th>detour</th>\n",
       "      <th>arrDay</th>\n",
       "      <th>...</th>\n",
       "      <th>stops_1.0</th>\n",
       "      <th>stops_2.0</th>\n",
       "      <th>segn_1.0</th>\n",
       "      <th>segn_2.0</th>\n",
       "      <th>segn_3.0</th>\n",
       "      <th>real_dist</th>\n",
       "      <th>dep_hour</th>\n",
       "      <th>dep_min</th>\n",
       "      <th>arr_hour</th>\n",
       "      <th>arr_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>181</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1.02676</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3899.12</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>181</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1.02676</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3899.12</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>181</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>1.02676</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3899.12</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>181</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1.02676</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3899.12</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>181</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1.02676</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3899.12</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Orig  con1  con2  Dest  depDay2  depDay3  depDay  elaptime   detour   \n",
       "0    26    64   103   181      2.0      NaN     2.0     535.0  1.02676  \\\n",
       "1    26    64   103   181      4.0      NaN     4.0     535.0  1.02676   \n",
       "2    26    64   103   181      5.0      NaN     4.0     775.0  1.02676   \n",
       "3    26    64   103   181      5.0      NaN     5.0     535.0  1.02676   \n",
       "4    26    64   103   181      6.0      NaN     6.0     535.0  1.02676   \n",
       "\n",
       "   arrDay  ...  stops_1.0  stops_2.0  segn_1.0  segn_2.0  segn_3.0  real_dist   \n",
       "0     3.0  ...          1          0         0         1         0    3899.12  \\\n",
       "1     5.0  ...          1          0         0         1         0    3899.12   \n",
       "2     5.0  ...          1          0         0         1         0    3899.12   \n",
       "3     6.0  ...          1          0         0         1         0    3899.12   \n",
       "4     7.0  ...          1          0         0         1         0    3899.12   \n",
       "\n",
       "   dep_hour  dep_min  arr_hour  arr_min  \n",
       "0        15       45         1       40  \n",
       "1        15       45         1       40  \n",
       "2        15       45         5       40  \n",
       "3        15       45         1       40  \n",
       "4        15       45         1       40  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the preprocessed data\n",
    "df = pd.read_csv('C:/GitHub/pdsp2023/backend/dataset/dataprep_v2.csv').drop(columns=['Unnamed: 0'])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first preprocessed data & original dataset\n",
    "df_v1 = pd.read_csv('C:/GitHub/pdsp2023/backend/dataset/dataprep_v1.csv').drop(columns='Unnamed: 0')\n",
    "df_org = pd.read_csv('C:/GitHub/pdsp2023/backend/dataset/data_org.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Orig', 'con1_arr', 'con1', 'con2_arr', 'con2', 'Dest', 'mkt_airline1',\n",
       "       'mkt_flight1', 'op_airline1', 'op_flight1', 'depDay1', 'd_time_1',\n",
       "       'a_time_1', 'depTime_U1', 'arrTime_U1', 'acrt1', 'distSeg1',\n",
       "       'mkt_airline2', 'mkt_flight2', 'op_airline2', 'op_flight2', 'depDay2',\n",
       "       'd_time_2', 'a_time_2', 'depTime_U2', 'arrTime_U2', 'acrt2', 'distSeg2',\n",
       "       'mkt_airline3', 'mkt_flight3', 'op_airline3', 'op_flight3', 'depDay3',\n",
       "       'd_time_3', 'a_time_3', 'depTime_U3', 'arrTime_U3', 'acrt3', 'distSeg3',\n",
       "       'dist', 'segn', 'week', 'depDay', 'deptime', 'deptime_U', 'arrtime',\n",
       "       'arrtime_U', 'elaptime', 'detour', 'arrDay', 'stops', 'dmcc', 'nLegs',\n",
       "       'paxe', 'cluster', 'OnD', 'TOT_pax', 'market_share'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the total waiting time for connection\n",
    "df_org.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       0\n",
       "1                       0\n",
       "2                       0\n",
       "3                       0\n",
       "4                       0\n",
       "               ...       \n",
       "522801                  0\n",
       "522802                  0\n",
       "522803                  0\n",
       "522804    0 days 03:37:00\n",
       "522805    0 days 04:18:00\n",
       "Name: con_time, Length: 522806, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_org['depTime_U2'] = pd.to_datetime(df_org['depTime_U2'])\n",
    "# df_org['arrTime_U1'] = pd.to_datetime(df_org['arrTime_U1'])\n",
    "# df_org['depTime_U3'] = pd.to_datetime(df_org['depTime_U3'])\n",
    "# df_org['arrTime_U2'] = pd.to_datetime(df_org['arrTime_U2'])\n",
    "\n",
    "# time for connection\n",
    "# df['con_time'] = (df_org['depTime_U2']- df_org['arrTime_U1']) + (df_org['depTime_U3'] - df_org['arrTime_U2'])\n",
    "df['con_time'] = df['con_time'].fillna(0)\n",
    "df['con_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Orig', 'con1', 'con2', 'Dest', 'depDay2', 'depDay3', 'depDay',\n",
      "       'elaptime', 'detour', 'arrDay', 'paxe', 'cluster', 'TOT_pax',\n",
      "       'market_share', 'is_direct_flight', 'stops_0.0', 'stops_1.0',\n",
      "       'stops_2.0', 'segn_1.0', 'segn_2.0', 'segn_3.0', 'real_dist',\n",
      "       'dep_hour', 'dep_min', 'arr_hour', 'arr_min', 'con_time'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A simpler model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orig</th>\n",
       "      <th>con1</th>\n",
       "      <th>con2</th>\n",
       "      <th>Dest</th>\n",
       "      <th>depDay2</th>\n",
       "      <th>depDay3</th>\n",
       "      <th>depDay</th>\n",
       "      <th>elaptime</th>\n",
       "      <th>detour</th>\n",
       "      <th>arrDay</th>\n",
       "      <th>...</th>\n",
       "      <th>stops_2.0</th>\n",
       "      <th>segn_1.0</th>\n",
       "      <th>segn_2.0</th>\n",
       "      <th>segn_3.0</th>\n",
       "      <th>real_dist</th>\n",
       "      <th>dep_hour</th>\n",
       "      <th>dep_min</th>\n",
       "      <th>arr_hour</th>\n",
       "      <th>arr_min</th>\n",
       "      <th>con_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>181</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1.02676</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3899.12</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>181</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1.02676</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3899.12</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>181</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>1.02676</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3899.12</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>181</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1.02676</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3899.12</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>181</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>1.02676</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3899.12</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Orig  con1  con2  Dest  depDay2  depDay3  depDay  elaptime   detour   \n",
       "0    26    64   103   181      2.0      NaN     2.0     535.0  1.02676  \\\n",
       "1    26    64   103   181      4.0      NaN     4.0     535.0  1.02676   \n",
       "2    26    64   103   181      5.0      NaN     4.0     775.0  1.02676   \n",
       "3    26    64   103   181      5.0      NaN     5.0     535.0  1.02676   \n",
       "4    26    64   103   181      6.0      NaN     6.0     535.0  1.02676   \n",
       "\n",
       "   arrDay  ...  stops_2.0  segn_1.0  segn_2.0  segn_3.0  real_dist  dep_hour   \n",
       "0     3.0  ...          0         0         1         0    3899.12        15  \\\n",
       "1     5.0  ...          0         0         1         0    3899.12        15   \n",
       "2     5.0  ...          0         0         1         0    3899.12        15   \n",
       "3     6.0  ...          0         0         1         0    3899.12        15   \n",
       "4     7.0  ...          0         0         1         0    3899.12        15   \n",
       "\n",
       "   dep_min  arr_hour  arr_min  con_time  \n",
       "0       45         1       40       NaT  \n",
       "1       45         1       40       NaT  \n",
       "2       45         5       40       NaT  \n",
       "3       45         1       40       NaT  \n",
       "4       45         1       40       NaT  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # of connections & how much time we should wait for connecting flights\n",
    "X = df.loc[:, ['Orig', 'con1', 'con2', 'Dest', 'depDay2', 'depDay3', 'depDay',\n",
    "       'elaptime', 'detour', 'arrDay', 'cluster',\n",
    "       'market_share', 'is_direct_flight', 'stops_0.0', 'stops_1.0',\n",
    "       'stops_2.0', 'segn_1.0', 'segn_2.0', 'segn_3.0', 'real_dist',\n",
    "       'dep_hour', 'dep_min', 'arr_hour', 'arr_min', 'con_time']]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    0.0\n",
       "Name: paxe, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Didn't include\n",
    "#  'TOT_pax',\n",
    "y = df.loc[:, 'paxe']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Splitting the dataset into training/test set\n",
    "https://builtin.com/data-science/random-forest-python<br/>\n",
    "https://machinelearningmastery.com/random-forest-ensemble-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (418244, 25)\n",
      "Shape of y_train: (418244,)\n",
      "Shape of X_test: (104562, 25)\n",
      "Shape of y_test: (104562,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# random_state: controls the shuffling applied to the data before applying the split. \n",
    "# Pass an int for reproducible output across multiple function calls.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# check the size of the splitted dataset\n",
    "print(\"Shape of X_train:\", X_train.shape) # expected output (418244, 22)\n",
    "print(\"Shape of y_train:\", y_train.shape) # expected output (418244,)\n",
    "print(\"Shape of X_test:\", X_test.shape) # expected output (104562, 22)\n",
    "print(\"Shape of y_test:\", y_test.shape) # expected output (104562,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The DType <class 'numpy.dtype[timedelta64]'> could not be promoted by <class 'numpy.dtype[float64]'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[timedelta64]'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\GitHub\\pdsp2023\\backend\\ML\\rf.ipynb Cell 14\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GitHub/pdsp2023/backend/ML/rf.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# imp = SimpleImputer(missing_values=np.nan, strategy='mean')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GitHub/pdsp2023/backend/ML/rf.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# imp = imp.fit(X_train)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GitHub/pdsp2023/backend/ML/rf.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# X_train = imp.transform(X_train)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GitHub/pdsp2023/backend/ML/rf.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m imputer \u001b[39m=\u001b[39m KNNImputer(n_neighbors\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, weights\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/GitHub/pdsp2023/backend/ML/rf.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X_train \u001b[39m=\u001b[39m imputer\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHub/pdsp2023/backend/ML/rf.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Standardisation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHub/pdsp2023/backend/ML/rf.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n",
      "File \u001b[1;32mc:\\Users\\0youn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\0youn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\0youn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\impute\\_knn.py:226\u001b[0m, in \u001b[0;36mKNNImputer.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     force_all_finite \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 226\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    227\u001b[0m     X,\n\u001b[0;32m    228\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    229\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    230\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    231\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy,\n\u001b[0;32m    232\u001b[0m )\n\u001b[0;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_X \u001b[39m=\u001b[39m X\n\u001b[0;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mask_fit_X \u001b[39m=\u001b[39m _get_mask(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing_values)\n",
      "File \u001b[1;32mc:\\Users\\0youn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\0youn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:778\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    774\u001b[0m     pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[0;32m    775\u001b[0m         _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[0;32m    776\u001b[0m     )\n\u001b[0;32m    777\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 778\u001b[0m         dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mresult_type(\u001b[39m*\u001b[39;49mdtypes_orig)\n\u001b[0;32m    780\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    781\u001b[0m     \u001b[39m# array is a pandas series\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     pandas_requires_conversion \u001b[39m=\u001b[39m _pandas_dtype_needs_early_conversion(array\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The DType <class 'numpy.dtype[timedelta64]'> could not be promoted by <class 'numpy.dtype[float64]'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[int64]'>, <class 'numpy.dtype[timedelta64]'>)"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# imp = imp.fit(X_train)\n",
    "# X_train = imp.transform(X_train)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2, weights='uniform')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "\n",
    "# Standardisation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating a random forest regression model and fitting it to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#### Later, can run a loop to find the best n_estimators ####\n",
    "# Create a random forest regressor\n",
    "forest = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "\n",
    "# Fit the model to the training data\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target variable on the training data\n",
    "y_train_pred = forest.predict(X_train)\n",
    "\n",
    "# Predict the target variable on the test data\n",
    "y_test_pred = forest.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate Model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_rf_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_rf_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Report the result\n",
    "print('Random Forest')\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_rf_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_rf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[:10])\n",
    "print(y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Parameters in use for basic model above\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(forest.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features}\n",
    "               #'max_depth': max_depth,\n",
    "               #'min_samples_split': min_samples_split,\n",
    "               #'min_samples_leaf': min_samples_leaf,\n",
    "               #'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random search training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate GPU\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '/device:GPU:0':\n",
    "  print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "  print('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 3 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf,\n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 3, cv = 3, verbose=2, \n",
    "                               random_state=101, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the best parameters from fitting the random search:\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Validate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate the random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import average_precision_score, accuracy_score\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "  y_pred = model.predict(X_test)\n",
    "  error = np.mean((y_pred-y_test) / y_test) * 100\n",
    "  accuracy = 100 - error\n",
    "  print('Model Performance')\n",
    "  print('Average Error: {:0.4f} degrees.'.format(error))\n",
    "  print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "  return accuracy\n",
    "\n",
    "# Base Model Performance    \n",
    "base_model = RandomForestRegressor(random_state=101, n_estimators=10)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search Model performance\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# plt.scatter(X_test, y_test)\n",
    "# plt.scatter(X_test, y_test, color='red')\n",
    "# plt.plot(X_test, forest.predict(X_test))\n",
    "# plt.title('Traffic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Predict pax for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Regressor\n",
    "#rf = RandomForestRegressor(random_state=101, \n",
    "                           #n_estimators=1800)\n",
    "\n",
    "# Fit the Random Forest Regressor to the training data\n",
    "#rf.fit(X_train_std, y_train)\n",
    "\n",
    "# get the predicted y values for the test set\n",
    "rf_pred_test_new = rf.predict(X_test_new_std)\n",
    "\n",
    "print(\"Random Forest predicted values for X_test:\", rf_pred_test_new)\n",
    "\n",
    "# get the id column from the original dataframe\n",
    "id_column = rentCH_test['id'].reset_index(drop=True)\n",
    "\n",
    "# create dataframes for predicted values with id column\n",
    "df_rf = pd.concat([id_column, pd.DataFrame(rf_pred_test_new, columns=[\"Random Forest Predictions\"])], axis=1)\n",
    "\n",
    "# save dataframes as CSV files\n",
    "df_rf.to_csv(\"rf_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out multioutput regressor\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html#sklearn.multioutput.MultiOutputRegressor\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_regression_multioutput.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape[0] * 0.8)\n",
    "print(X.shape[0] * 0.2)\n",
    "print(418245+104561 == X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xrnd = np.sort(200 * np.random.rand(600, 1) - 100, axis=0)\n",
    "yrnd = np.array([np.pi * np.sin(Xrnd).ravel(), np.pi * np.cos(Xrnd).ravel()]).T\n",
    "yrnd += 0.5 - np.random.rand(*yrnd.shape)\n",
    "print(Xrnd.shape)\n",
    "print(yrnd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# input: dep_date, arr_date\n",
    "# output: dep_hour, dep_min, arr_hour, arr_min, pax, elapsetime, real_dist\n",
    "\n",
    "X_multirf = df\n",
    "y_multirf = df.loc[:, ['real_dist', 'dep_date', 'dep_hour', 'dep_min', \n",
    "                   'arr_date', 'arr_hour', 'arr_min']]\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_multirf, y_multirf, \n",
    "                                                train_size=418245, test_size=104561, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Create a random dataset\n",
    "max_depth = 30\n",
    "regr_multirf = MultiOutputRegressor(\n",
    "    RandomForestRegressor(n_estimators=100, max_depth=max_depth, random_state=0)\n",
    ")\n",
    "regr_multirf.fit(Xtrain, ytrain)\n",
    "\n",
    "# Predict on new data\n",
    "y_multirf = regr_multirf.predict(Xtest)\n",
    "y_multirf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
